---
title: 【爬虫】大学排名、国家院士
tags:
  - 爬虫
  - python
---

## 第一项内容:爬取大学排名

> 爬取最好大学网网页中549所国内大学的排名数据，并将排名、学校名称、省市、总分、培养规模打印出来。 http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html

```python
import pandas as pd
from bs4 import BeautifulSoup
import os
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'
}

res_list = []

ul = 'http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html'
html = requests.get(ul, headers=headers)
html.encoding = 'utf-8'  # 转下编码
html = html.text
soup = BeautifulSoup(html, "lxml")
note_list = soup.find_all('tr', {'class': 'alt'})
for note in note_list:
    information = note.find_all('td')  # 读取要用的东西
    rank = information[0].text  # 解包
    school = information[1].text
    location = information[2].text
    all_score = information[3].text
    source_score = information[4].text
    society = information[6].text
    res_list.append([rank, school, location, all_score, source_score, society])
for i in res_list:
    for j in i:
        print(j, end="     ")
    print('')

```


## 第二项内容:爬取中国工程院院士信息

> 爬取中国工程院网页上，把每位院士的简介保存为本地文件，把每位院士的照片保存为本地图片，文本文件和图片文件都以院士的姓名为主文件。http://www.cae.cn/cae/html/main/col48/column_48_1.html 

```python
import pandas as pd
from bs4 import BeautifulSoup
import re
import os
import requests

headers = {  # 浏览器标识
    'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'
}

ul = 'http://www.cae.cn/cae/html/main/col48/column_48_1.html'
html = requests.get(ul, headers=headers)
html.encoding = 'utf-8'
html = html.text
soup = BeautifulSoup(html, "lxml")
note_list = soup.find_all('li', {'class': 'name_list'})
pattern = '<a href="(.+)" target="_blank">(.+)</a>'  # 准备好正则表达式
pattern2 = '<img src="(.+)" style='

result = []
for note in note_list:
    result.append(re.findall(pattern, str(note))[0])  # 把网址是名字搞下来

for website in result:
    print('正在爬' + website[1])

    ul = 'http://www.cae.cn/' + website[0]  # 补充完整网址
    article = requests.get(ul, headers=headers)
    article.encoding = 'utf-8'
    soup = BeautifulSoup(article.text, 'lxml')

    img = soup.find('div', {'class': 'info_img'})
    img_place = re.findall(pattern2, str(img))
    img_ul = 'http://www.cae.cn/' + img_place[0]  # 补充完整的图片网址

    res = ''
    intro = soup.find('div', {'class': 'intro'})
    paragraph = intro.find_all('p')
    for i in paragraph:
        res += i.text + '\n'  # 把个人简介弄出来

    if not os.path.exists('F:\spider\\' + website[1] + '\\'):  # 判断文件夹是否存在
        os.makedirs('F:\spider\\' + website[1] + '\\')  # 创建文件夹

    f = open('F:\spider\\' + website[1] + '\\' + website[1] + '.txt', 'w+', encoding='utf-8')  # 存文本
    f.write(res)
    f.close()

    f = open('F:\spider\\' + website[1] + '\\' + website[1] + '.jpg', 'wb')
    f.write(requests.get(img_ul).content)  # 存图片
    f.close()

```